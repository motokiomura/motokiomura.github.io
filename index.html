<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Motoki Omura</title>

    <meta name="author" content="Motoki Omura">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Motoki Omura
                </p>
                <p>
    I am a Research Scientist at <a href="https://www.sbintuitions.co.jp/en/">SB Intuitions</a>, where I work on VLA models in the Robotics team. I received my Ph.D. from the University of Tokyo, where I was advised by <a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Tatsuya Harada</a> and <a href="https://takaosa.github.io/about.html">Takayuki Osa</a>. 
    <!-- Previously, I interned at DeNA, Preferred Networks, and Integral AI. -->
    <br><br>
    My research focuses on <strong>reinforcement learning (RL) algorithms</strong>, particularly on improving stability and efficiency in both online and offline settings. I apply these techniques to domains such as robotics and LLM alignment (e.g., DPO).
                </p>
                <p style="text-align:center">
                  <a href="mailto:motoki.omura@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://x.com/motokiomura">X (Twitter)</a> &nbsp;/&nbsp;
                  <a href="https://github.com/motokiomura">Github</a> &nbsp;/&nbsp;
                  <a href="https://jp.linkedin.com/in/motoki-omura-396714211">LinkedIn</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile.png"><img style="width:60%;max-width:60%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps</span>
                </a>
                <br>
                <strong>Motoki Omura</strong>, Yusuke Mukuta, Kazuki Ota, Takayuki Osa, Tatsuya Harada 
                <br>
                <em>RLC</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2507.10843">paper</a> | 
                <a href="https://github.com/motokiomura/Q-DOT">code</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">Entropy Controllable Direct Preference Optimization</span>
                </a>
                <br>
                <strong>Motoki Omura</strong>, Yasuhiro Fujita, Toshiki Kataoka
                <br>
                <em>ICML</em>, 2025, <em>Workshop on Models of Human Feedback for AI Alignment</em>
                <br>
                <a href="https://arxiv.org/abs/2411.07595">paper</a> 
                <p></p>
              </td>
            </tr>

            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">Gradual Transition from Bellman Optimality Operator to Bellman Operator in Online Reinforcement Learning</span>
                </a>
                <br>
                <strong>Motoki Omura</strong>,  Takayuki Osa, Yusuke Mukuta, Tatsuya Harada 
                <br>
                <em>ICML</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2406.04896">paper</a> | 
                <a href="https://github.com/motokiomura/annealed-q-learning">code</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">Curriculum Reinforcement Learning in High-Dimensional Contextual Spaces and Its Application to Robotic Piano Playing</span>
                </a>
                <br>
                Haruki Abe, Takayuki Osa, <strong>Motoki Omura</strong>, Jen-Yen Chang, Tatsuya Harada
                <br>
                <em>Humanoids</em>, 2025, (Oral)
                <br>
                <a href="https://arxiv.org/abs/2507.10843">paper</a> | 
                <a href="https://github.com/motokiomura/Q-DOT">code</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">Stabilizing Extreme Q-learning by Maclaurin Expansion</span>
                </a>
                <br>
                <strong>Motoki Omura</strong>, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada 
                <br>
                <em>RLC</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2406.04896">paper</a> | 
                <a href="https://github.com/motokiomura/MXQL">code</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">Symmetric Q-Learning: Reducing Skewness of Bellman Error in Online Reinforcement Learning</span>
                </a>
                <br>
                <strong>Motoki Omura</strong>, Takayuki Osa, Yusuke Mukuta, Tatsuya Harada 
                <br>
                <em>AAAI</em>, 2024
                <br>
                <a href="https://arxiv.org/abs/2403.07704">paper</a>
                <p></p>
              </td>
            </tr>

            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">Model Compression and Acceleration by Combining Reinforcement Learning and Meta-Pruning</span>
                </a>
                <br>
                Yu Kono, <strong>Motoki Omura</strong>, Tomohiro Kato, Yusuke Uchida
                <br>
                <em>JSAI</em>, 2024
                <br>
                <a href="https://cir.nii.ac.jp/crid/1390848250119547136">paper</a>
                <p></p>
              </td>
            </tr>
  
            </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Awards</h2>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr onmouseout="nexf_stop()" onmouseover="nexf_start()">
              <td style="padding:16px;width:0%;vertical-align:middle">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://m-niemeyer.github.io/nexf/">
                  <span class="papertitle">The 5th place and Honorable Mention in “The Multi-Agent Reinforcement Learning in MalmÖ (MARLÖ) Competition 2019”</span>
                </a>
                <br>
                <strong>Motoki Omura</strong>
                <br>
                <a href="https://www.aicrowd.com/challenges/marlo-2018">Competiton page</a> | 
                <a href="https://www.microsoft.com/en-us/research/blog/winners-announced-in-multi-agent-reinforcement-learning-challenge/">Winners announcement</a>
                <p></p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding: 10px;">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    <a href="https://github.com/jonbarron/website">Template</a>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody></table>
    </body>
</html>